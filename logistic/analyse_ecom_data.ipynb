{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from process import get_data\n",
    "\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions\n",
    "def softmax(a):\n",
    "    expA = np.exp(a)\n",
    "    return expA / expA.sum(axis=1, keepdims=True)\n",
    "\n",
    "def forward(X, W, b):\n",
    "    return softmax(X.dot(W) + b)\n",
    "\n",
    "def predict(P_Y_given_X):\n",
    "    return np.argmax(P_Y_given_X, axis=1)\n",
    "\n",
    "# calculate the accuracy\n",
    "def classification_rate(Y, P):\n",
    "    return np.mean(Y == P)\n",
    "\n",
    "def cross_entropy(T, pY):\n",
    "    return -np.mean(T*np.log(pY))\n",
    "\n",
    "def y2indicator(y, K):\n",
    "    N = len(y)\n",
    "    ind = np.zeros((N, K))\n",
    "    for i in range(N):\n",
    "        ind[i, y[i]] = 1\n",
    "    return ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.598971269012886 0.6873872831701937\n",
      "1000 0.08339104446512106 0.112233053587592\n",
      "2000 0.07964342150795396 0.10640267754307384\n",
      "3000 0.07819091405856265 0.10439145098912775\n",
      "4000 0.07745713098200474 0.10343170597517705\n",
      "5000 0.07703179446269952 0.1028849732528845\n",
      "6000 0.07676302055579166 0.10253598857851372\n",
      "7000 0.07658274718928196 0.10229515364621283\n",
      "8000 0.07645642260049638 0.10211945502210419\n",
      "9000 0.07636487604121932 0.10198594000305038\n",
      "Final train classification_rate: 0.915\n",
      "Final test classification_rate: 0.91\n"
     ]
    }
   ],
   "source": [
    "W = np.random.randn(D, K)\n",
    "b = np.zeros(K)\n",
    "\n",
    "Xtrain, Ytrain, Xtest, Ytest = get_data()\n",
    "D = Xtrain.shape[1]\n",
    "K = len(set(Ytrain) | set(Ytest))\n",
    "\n",
    "# convert to indicator\n",
    "Ytrain_ind = y2indicator(Ytrain, K)\n",
    "Ytest_ind = y2indicator(Ytest, K)\n",
    "\n",
    "train_costs = []\n",
    "test_costs = []\n",
    "learning_rate = 0.001\n",
    "for i in range(10000):\n",
    "    pYtrain = forward(Xtrain, W, b)\n",
    "    pYtest = forward(Xtest, W, b)\n",
    "\n",
    "    ctrain = cross_entropy(Ytrain_ind, pYtrain)\n",
    "    ctest = cross_entropy(Ytest_ind, pYtest)\n",
    "    train_costs.append(ctrain)\n",
    "    test_costs.append(ctest)\n",
    "\n",
    "    # gradient descent\n",
    "    W -= learning_rate*Xtrain.T.dot(pYtrain - Ytrain_ind)\n",
    "    b -= learning_rate*(pYtrain - Ytrain_ind).sum(axis=0)\n",
    "    if i % 1000 == 0:\n",
    "        print(i, ctrain, ctest)\n",
    "\n",
    "print(\"Final train classification_rate:\", classification_rate(Ytrain, predict(pYtrain)))\n",
    "print(\"Final test classification_rate:\", classification_rate(Ytest, predict(pYtest)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoder(data):\n",
    "    # One-hot encoding\n",
    "    unique_time = np.unique(data)\n",
    "    #print(unique_time)\n",
    "    one_hot = np.zeros((data.shape[0], len(unique_time)))\n",
    "    for t in unique_time:\n",
    "        one_hot[:,int(t)] = np.where(data==t, 1, 0)\n",
    "        \n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_ant():\n",
    "    df = pd.read_csv('ecommerce_data.csv')\n",
    "    data = df.as_matrix()\n",
    "    \n",
    "    X = data[:,:-1]\n",
    "    Y = data[:,-1].astype(np.int32)\n",
    "    \n",
    "    X, Y = shuffle(X, Y, random_state=42)\n",
    "    \n",
    "    N, D = X.shape\n",
    "    \n",
    "    \n",
    "    # One-hot encoding\n",
    "    X2 = np.zeros((N,D+3))\n",
    "    X2[:,:D-1] = X[:,:D-1]\n",
    "    X2[:,D-1:D+3] = one_hot_encoder(X[:,D-1])\n",
    "    X = X2\n",
    "\n",
    "    X_train = X[:-100,:]\n",
    "    Y_train = Y[:-100]\n",
    "    X_test = X[-100:,:]\n",
    "    Y_test = Y[-100:]\n",
    "    \n",
    "    \n",
    "    # normalize the data\n",
    "    for i in (1,2):\n",
    "        X_train[:,i] = (X_train[:,i] - X_train[:,i].mean())/X_train[:,i].std()\n",
    "        X_test[:,i] = (X_test[:,i] - X_test[:,i].mean())/X_test[:,i].std()\n",
    "        \n",
    "    return X_train, Y_train, X_test, Y_test\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_ant(a):\n",
    "    exp_a = np.exp(a)\n",
    "    return exp_a/exp_a.sum(axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_rate_ant(Y, Y_hat_class):\n",
    "    return 100*np.mean(Y==Y_hat_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_ant(X, W, b):\n",
    "    return softmax(X.dot(W) + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General multi-class cross-entropy\n",
    "def cross_entropy_ant(Y, Y_hat):\n",
    "    return -np.mean(Y*np.log(Y_hat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_ant(Y_hat):\n",
    "    return np.argmax(Y_hat, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dJ_dw(Y, Y_hat, X):\n",
    "    return X.T.dot(Y-Y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def derivative_b(Y, Y_hat):\n",
    "    return (Y - Y_hat).sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400, 8) (400,) (100, 8) (100,)\n"
     ]
    }
   ],
   "source": [
    "X_train, Y_train, X_test, Y_test = get_data()\n",
    "print(X_train.shape, Y_train.shape, X_test.shape, Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = X_train.shape[1] # number of features\n",
    "K = len(set(Y_train)) # number of classes\n",
    "\n",
    "T_train = one_hot_encoder(Y_train)\n",
    "T_test = one_hot_encoder(Y_test)\n",
    "\n",
    "W = np.random.randn(D, K)\n",
    "b = np.random.randn(K)\n",
    "\n",
    "# Check\n",
    "# P_Y_given_x = forward(X_train, W, b)\n",
    "# Y_hat = predict(P_Y_given_x)\n",
    "# print('Classification rate:', np.round(classification_rate(Y_train, Y_hat),4), '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.8087971278692129 0.8658409464369052\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\dev\\anaconda\\envs\\env1\\lib\\site-packages\\ipykernel_launcher.py:3: RuntimeWarning: divide by zero encountered in log\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "D:\\dev\\anaconda\\envs\\env1\\lib\\site-packages\\ipykernel_launcher.py:3: RuntimeWarning: invalid value encountered in multiply\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "D:\\dev\\anaconda\\envs\\env1\\lib\\site-packages\\ipykernel_launcher.py:3: RuntimeWarning: overflow encountered in exp\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "D:\\dev\\anaconda\\envs\\env1\\lib\\site-packages\\ipykernel_launcher.py:4: RuntimeWarning: invalid value encountered in true_divide\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 nan nan\n",
      "2000 nan nan\n",
      "3000 nan nan\n",
      "4000 nan nan\n",
      "5000 nan nan\n",
      "6000 nan nan\n",
      "7000 nan nan\n",
      "8000 nan nan\n",
      "9000 nan nan\n",
      "classification_rate: 0.5075\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-3\n",
    "costs_train = []\n",
    "costs_test = []\n",
    "for epoch in range(10000):\n",
    "    Y_hat_train = forward_ant(X_train, W, b)\n",
    "    Y_hat_test = forward_ant(X_test, W, b)\n",
    "    \n",
    "    ctrain = cross_entropy_ant(T_train, Y_hat_train)\n",
    "    ctest = cross_entropy_ant(T_test, Y_hat_test)\n",
    "    \n",
    "    costs_train.append(ctrain)\n",
    "    costs_test.append(ctest)\n",
    "    \n",
    "    W -= learning_rate * dJ_dw(T_train, Y_hat_train, X_train)\n",
    "    b -= learning_rate * derivative_b(T_train, Y_hat_train)\n",
    "    \n",
    "    if epoch % 1000 == 0:\n",
    "        print(epoch, ctrain, ctest)\n",
    "\n",
    "\n",
    "    \n",
    "print(\"classification_rate:\", classification_rate(Y_train, predict_ant(Y_hat_train)))    \n",
    "# plt.plot(costs_train)\n",
    "# plt.plot(costs_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.49017228482615977, nan, nan, nan, nan, nan, nan, nan, nan, nan]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "costs_train\n",
    "costs_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def y2indicator(y, K):\n",
    "    N = len(y)\n",
    "    ind = np.zeros((N, K))\n",
    "    for i in range(N):\n",
    "        ind[i, y[i]] = 1\n",
    "    return ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400, 4)\n"
     ]
    }
   ],
   "source": [
    "Xtrain, Ytrain, Xtest, Ytest = get_data()\n",
    "D = Xtrain.shape[1]\n",
    "K = len(set(Ytrain) | set(Ytest))\n",
    "\n",
    "# convert to indicator\n",
    "Ytrain_ind = y2indicator(Ytrain, K)\n",
    "Ytest_ind = y2indicator(Ytest, K)\n",
    "\n",
    "print(Ytrain_ind.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 5 2\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.36432160804020103\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
